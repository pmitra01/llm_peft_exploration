{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c53c27ca",
   "metadata": {},
   "source": [
    "# Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8032e2",
   "metadata": {},
   "source": [
    "Things to do:\n",
    "do you want to use collate_fn in data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebef213",
   "metadata": {},
   "source": [
    "### imports and globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d956186",
   "metadata": {},
   "outputs": [],
   "source": [
    "T5_SMALL = \"t5-small\"\n",
    "GPT = \"gpt2\"  # 117M parameters as per https://huggingface.co/transformers/v3.3.1/pretrained_models.html # \"openai-gpt\"\n",
    "DISTILBERT = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71800546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt\n",
    "from collections import defaultdict\n",
    "\n",
    "# mandatory imports\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.data import random_split\n",
    "import collections\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98a779",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae590d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SIZE = 0.1\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "METRIC_NAME = \"f1\"\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a90ce66",
   "metadata": {
    "tags": []
   },
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c580ec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ag_news (/home/jovyan/llm_peft_exploration/notebooks/datasets/.cache/huggingface_datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a284bed52c428aa5481164b16261b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#raw_dataset = load_dataset('super_glue', 'cb', cache_dir=\"./datasets/.cache/huggingface_datasets\")\n",
    "raw_dataset = load_dataset('ag_news', cache_dir=\"./datasets/.cache/huggingface_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f67202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\"\n",
    "}\n",
    "\n",
    "label2id = {v:k for k,v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87072172",
   "metadata": {
    "tags": []
   },
   "source": [
    "### explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0669ab23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 120000, 'test': 7600}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: len(raw_dataset[k]) for k in raw_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57d45cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\",\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['test'][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13978a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc660581",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84eb9d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(GPT)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'  # 'left'\n",
    "\n",
    "# Use `DataCollatorWithPadding` as it is more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding to max length\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a96d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    # take a batch of texts\n",
    "    text = examples[\"text\"]  # this has n_rows which = batch_size\n",
    "    # encode them\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors='pt')\n",
    "    # add labels\n",
    "    labels_batch = torch.tensor(examples['label'])\n",
    "    #torch.transpose(labels_batch, 0, 1)\n",
    "    # create numpy array of shape (batch_size, num_labels)\n",
    "    labels_matrix = torch.nn.functional.one_hot(labels_batch)\n",
    "    labels_matrix = labels_matrix.float()  # without converting int to float, you get an error later\n",
    "    # print(labels_matrix)\n",
    "    encoding[\"label\"] = labels_matrix#.tolist()\n",
    "  \n",
    "    return encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6812b",
   "metadata": {},
   "source": [
    "#### encode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "becd5e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize \n",
    "#encoded_dataset = raw_dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2e68ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(ds, tokenizer, create_validation_split=False, validation_prop=0.1):\n",
    "    encoded_dataset = ds.map(preprocess_data, batched=True)\n",
    "    # is this needed?\n",
    "    encoded_dataset.set_format(\"torch\")\n",
    "    \n",
    "    # we need to create tratin/valid sets\n",
    "    print(f\"creating validation split: {str(create_validation_split)}\")\n",
    "    if create_validation_split:\n",
    "        train_dataset, validation_dataset= encoded_dataset['train'].train_test_split(test_size=validation_prop).values()\n",
    "    return {'train': train_dataset, 'valid': validation_dataset, 'test': encoded_dataset['test']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "138b2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_label_distr_from_data_loader(dl):\n",
    "    label_list = []\n",
    "    for idx, item in enumerate(dl):\n",
    "        batch_labels_onehot = item['label']#.tolist()\n",
    "        batch_labels = [torch.argmax(label_onehot).item() for label_onehot in batch_labels_onehot]\n",
    "        label_list.extend(batch_labels)\n",
    "    print(f'distribution of labels: {collections.Counter(label_list)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7e7e34d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/llm_peft_exploration/notebooks/datasets/.cache/huggingface_datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-c4f7b776196c8f42.arrow\n",
      "Loading cached processed dataset at /home/jovyan/llm_peft_exploration/notebooks/datasets/.cache/huggingface_datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-43f61cb9fd51c4df.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating validation split: True\n",
      "distribution of labels: Counter({0: 3067, 2: 3028, 1: 2963, 3: 2942})\n",
      "distribution of labels: Counter({3: 27058, 1: 27037, 2: 26972, 0: 26933})\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = prepare_dataset(raw_dataset, tokenizer, create_validation_split=True)\n",
    "valid_loader = DataLoader(encoded_dataset['valid'], batch_size=BATCH_SIZE, shuffle=True)#, collate_fn=data_collator)\n",
    "train_loader = DataLoader(encoded_dataset['train'], batch_size=BATCH_SIZE, shuffle=True)#, collate_fn=data_collator)\n",
    "\n",
    "describe_label_distr_from_data_loader(valid_loader)\n",
    "describe_label_distr_from_data_loader(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3491f1",
   "metadata": {},
   "source": [
    "### tokenise, training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2d43820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = Path(f\"data/models_20230606\")\n",
    "MODEL_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f2af02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(GPT, \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(label2id),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab032677",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EvalPrediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     15\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m: f1_micro_average,\n\u001b[1;32m     16\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m: roc_auc,\n\u001b[1;32m     17\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy}\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metrics\u001b[39m(p: \u001b[43mEvalPrediction\u001b[49m):\n\u001b[1;32m     21\u001b[0m     preds \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mpredictions[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(p\u001b[38;5;241m.\u001b[39mpredictions, \n\u001b[1;32m     22\u001b[0m             \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m p\u001b[38;5;241m.\u001b[39mpredictions\n\u001b[1;32m     23\u001b[0m     result \u001b[38;5;241m=\u001b[39m multi_label_metrics(\n\u001b[1;32m     24\u001b[0m         predictions\u001b[38;5;241m=\u001b[39mpreds, \n\u001b[1;32m     25\u001b[0m         labels\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39mlabel_ids)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EvalPrediction' is not defined"
     ]
    }
   ],
   "source": [
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# consider std text clf that assumes more of a softmax process\n",
    "def compute_metrics_std(p: EvalPrediction):\n",
    "    predictions = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    labels = p.label_ids\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38f02c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SIZE = 0.1\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "METRIC_NAME = \"f1\"\n",
    "N_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "baeb44ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c212bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=MODEL_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=0.1,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=METRIC_NAME,\n",
    "    logging_dir='./logs',            # directory for storing logs*\n",
    "    logging_steps=2000,\n",
    "    report_to='wandb',\n",
    "    save_total_limit = 5,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35288bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['valid'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67145a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4eabfb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999fb350fd1047a695b935ff2eae60ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669471766666295, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/llm_peft_exploration/notebooks/wandb/run-20230804_183711-ae1vlmo1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/discoverylab/huggingface/runs/ae1vlmo1' target=\"_blank\">happy-mountain-9</a></strong> to <a href='https://wandb.ai/discoverylab/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/discoverylab/huggingface' target=\"_blank\">https://wandb.ai/discoverylab/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/discoverylab/huggingface/runs/ae1vlmo1' target=\"_blank\">https://wandb.ai/discoverylab/huggingface/runs/ae1vlmo1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='338' max='338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [338/338 02:14, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.139214</td>\n",
       "      <td>0.899705</td>\n",
       "      <td>0.930361</td>\n",
       "      <td>0.880583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=338, training_loss=0.23731443867880916, metrics={'train_runtime': 153.8877, 'train_samples_per_second': 70.181, 'train_steps_per_second': 2.196, 'total_flos': 706559172673536.0, 'train_loss': 0.23731443867880916, 'epoch': 0.1})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57066a81",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee50d484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.13921377062797546,\n",
       " 'eval_f1': 0.8997052631578947,\n",
       " 'eval_roc_auc': 0.9303611111111111,\n",
       " 'eval_accuracy': 0.8805833333333334,\n",
       " 'eval_runtime': 31.6581,\n",
       " 'eval_samples_per_second': 379.05,\n",
       " 'eval_steps_per_second': 11.845,\n",
       " 'epoch': 0.1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c909c0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.08296191692352295,\n",
       " 'eval_f1': 0.9416106497957031,\n",
       " 'eval_roc_auc': 0.9605701754385965,\n",
       " 'eval_accuracy': 0.9361842105263158}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.eval_dataset=encoded_dataset['test']\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ced62e",
   "metadata": {},
   "source": [
    "test how the accuracy improves with batches of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b3fccf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 108000\n",
       " }),\n",
       " 'num_workers': 0,\n",
       " 'prefetch_factor': 2,\n",
       " 'pin_memory': False,\n",
       " 'pin_memory_device': '',\n",
       " 'timeout': 0,\n",
       " 'worker_init_fn': None,\n",
       " '_DataLoader__multiprocessing_context': None,\n",
       " '_dataset_kind': 0,\n",
       " 'batch_size': 32,\n",
       " 'drop_last': False,\n",
       " 'sampler': <torch.utils.data.sampler.RandomSampler at 0x7fbba41b1550>,\n",
       " 'batch_sampler': <torch.utils.data.sampler.BatchSampler at 0x7fbba41b15e0>,\n",
       " 'generator': None,\n",
       " 'collate_fn': <function torch.utils.data._utils.collate.default_collate(batch)>,\n",
       " 'persistent_workers': False,\n",
       " '_DataLoader__initialized': True,\n",
       " '_IterableDataset_len_called': None,\n",
       " '_iterator': None}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcdd384",
   "metadata": {
    "tags": []
   },
   "source": [
    "### to delete batch sampling custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe686c2",
   "metadata": {},
   "source": [
    "https://www.scottcondron.com/jupyter/visualisation/audio/2020/12/02/dataloaders-samplers-collate.html#Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab815238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from torch.utils.data.sampler import Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f90414a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE = 10\n",
    "BATCH_SIZE = 32\n",
    "a1 = [np.power(BASE, i) for i in range(5)]\n",
    "math.log(1200, BASE)\n",
    "np.power(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25bd08a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(indices, chunk_size=-1):\n",
    "    if chunk_size<1:\n",
    "        chunk_size = len(indices)\n",
    "    return torch.split(torch.tensor(indices), chunk_size)\n",
    "\n",
    "class SequentialTrainingBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size, shuffle=False, base=10):\n",
    "        max_segments = int(math.log(len(dataset), base))\n",
    "        self.log_index_markers = [np.power(base, sgmt) for sgmt in range(1, max_segments+1)]\n",
    "        if max_segments < math.log(len(dataset)):\n",
    "            self.log_index_markers.append(len(dataset))\n",
    "        self.indices_lists = [list(range(marker)) for marker in self.log_index_markers]\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            for indices_list in self.indices_lists:\n",
    "                random.shuffle(indices_list)\n",
    "       ## print(self.indices_lists)\n",
    "        segment_batches  = [chunk(segment_indices_list, self.batch_size) for segment_indices_list in self.indices_lists]\n",
    "\n",
    "        #combined = [[batch.tolist() for batch in segment] for segment in segment_batches]\n",
    "        combined = [batch.tolist() for segment in segment_batches for batch in segment]\n",
    "\n",
    "        if self.shuffle:\n",
    "            random.shuffle(combined)\n",
    "        return iter(combined)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return  len(self.indices_lists)#sum([len(segment_indices) for segment_indices in self.indices_lists]) #// self.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12515f4c",
   "metadata": {},
   "source": [
    "### experiment - sequential supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5079f9fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number #0:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Batch number #1:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import BatchSampler, SequentialSampler, DataLoader\n",
    "BASE=10\n",
    "custom_sequential_sampler = SequentialTrainingBatchSampler(encoded_dataset['train'], batch_size=-1, base=BASE)\n",
    "for i, batch in enumerate(custom_sequential_sampler):\n",
    "    print(f\"Batch number #{i}:  {batch}\")\n",
    "    if i>=1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8d64fb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0), 6, PosixPath('data/models_20230606'))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device, len(custom_sequential_sampler), MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0c8af01",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data points: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a1414b532e4743af1a1f70c64da5a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669560833334648, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/llm_peft_exploration/notebooks/wandb/run-20230806_094509-wvk5npve</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/discoverylab/huggingface/runs/wvk5npve' target=\"_blank\">happy-oath-12</a></strong> to <a href='https://wandb.ai/discoverylab/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/discoverylab/huggingface' target=\"_blank\">https://wandb.ai/discoverylab/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/discoverylab/huggingface/runs/wvk5npve' target=\"_blank\">https://wandb.ai/discoverylab/huggingface/runs/wvk5npve</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.529575</td>\n",
       "      <td>0.342382</td>\n",
       "      <td>0.492472</td>\n",
       "      <td>0.000583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data points: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.358408</td>\n",
       "      <td>0.348142</td>\n",
       "      <td>0.500597</td>\n",
       "      <td>0.001083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data points: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.066795</td>\n",
       "      <td>0.359390</td>\n",
       "      <td>0.520014</td>\n",
       "      <td>0.016583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data points: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:56, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.487509</td>\n",
       "      <td>0.389117</td>\n",
       "      <td>0.609722</td>\n",
       "      <td>0.260667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data points: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 02:08, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.133472</td>\n",
       "      <td>0.899786</td>\n",
       "      <td>0.931403</td>\n",
       "      <td>0.877583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data points: 108000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='338' max='338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [338/338 02:18, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.116661</td>\n",
       "      <td>0.913438</td>\n",
       "      <td>0.941431</td>\n",
       "      <td>0.898500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7f79e7ce98b0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do for 10, 50, 100, 1000, 20000, 100000\n",
    "from torch.utils.data import Subset\n",
    "total_subset_idx = []\n",
    "sequential_supervision_val_scores = []\n",
    "sequential_supervision_test_scores = []\n",
    "for idx, idx_batch in enumerate(custom_sequential_sampler):\n",
    "    #if idx<4:\n",
    "    #total_subset_idx.extend(idx_batch)\n",
    "    batch_dataset = Subset(encoded_dataset['train'], idx_batch)\n",
    "    print(f\"Number of training data points: {len(idx_batch)}\")\n",
    "    #batch_loader = DataLoader(batch_dataset)\n",
    "    args = TrainingArguments(\n",
    "        output_dir=MODEL_DIR,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=0.1,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=METRIC_NAME,\n",
    "        logging_dir='./logs',            # directory for storing logs*\n",
    "        logging_steps=2000,\n",
    "        report_to='wandb',\n",
    "        save_total_limit = 5,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "                model,\n",
    "                args,\n",
    "                train_dataset=batch_dataset,\n",
    "                eval_dataset=encoded_dataset['valid'],\n",
    "                tokenizer=tokenizer,\n",
    "                compute_metrics=compute_metrics,\n",
    "                #data_collator=data_collator\n",
    "            )\n",
    "    trainer.train()\n",
    "    \n",
    "    print(f\"evaluating on validation set\")\n",
    "    trainer.eval_dataset=encoded_dataset['valid']\n",
    "    val_scores = trainer.evaluate()        \n",
    "    sequential_supervision_val_scores.append(val_scores)\n",
    "    \n",
    "    print(f\"evaluating on test set\")\n",
    "    trainer.eval_dataset=encoded_dataset['test']\n",
    "    test_scores = trainer.evaluate()\n",
    "    sequential_supervision_test_scores.append(test_scores)\n",
    "\n",
    "\n",
    "\n",
    "    #trainer.save()\n",
    "batch_dataset                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "79694f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0005833333333333334, 0.0010833333333333333, 0.016583333333333332, 0.26066666666666666, 0.8775833333333334, 0.8985]\n"
     ]
    }
   ],
   "source": [
    "print([score['eval_accuracy'] for score in sequential_supervision_val_scores])\n",
    "import json\n",
    "with open(MODEL_DIR.joinpath(\"val_scores.json\"), \"w+\") as f:\n",
    "    json.dump(sequential_supervision_val_scores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a249f22",
   "metadata": {},
   "source": [
    "##### push model to hf hub\n",
    "\n",
    "(https://colab.research.google.com/drive/1U7SX7jNYsNQG5BY1xEQQHu48Pn6Vgnyt?usp=sharing#scrollTo=H5j5YJE2hK58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b52171f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5ee05875f940e8af9d802355ad8d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748bbbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(\"your-username/model-name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b2b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39da19dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0655e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-bioblp-env [Python]",
   "language": "python",
   "name": "conda-env-.conda-bioblp-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
