{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c53c27ca",
   "metadata": {},
   "source": [
    "# Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "517e2e60-a8ac-43d0-af93-d6f6fdd9c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebef213",
   "metadata": {},
   "source": [
    "### imports and globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d956186",
   "metadata": {},
   "outputs": [],
   "source": [
    "T5_SMALL = \"t5-small\"\n",
    "GPT = \"gpt2\"  # 117M parameters as per https://huggingface.co/transformers/v3.3.1/pretrained_models.html # \"openai-gpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71800546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt\n",
    "from collections import defaultdict\n",
    "\n",
    "# mandatory imports\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.data import random_split\n",
    "import collections\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baeb44ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98a779",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae590d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SIZE = 0.1\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "METRIC_NAME = \"f1\"\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a90ce66",
   "metadata": {
    "tags": []
   },
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c580ec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ag_news (/home/jovyan/llm_peft_exploration/notebooks/datasets/.cache/huggingface_datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841ae0fdb3f64f8fbf714a891afa7adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset = load_dataset('ag_news', cache_dir=\"./datasets/.cache/huggingface_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f67202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\"\n",
    "}\n",
    "\n",
    "label2id = {v:k for k,v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87072172",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### explore shape of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0669ab23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 120000, 'test': 7600}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: len(raw_dataset[k]) for k in raw_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57d45cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\",\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['test'][0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc660581",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocess dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249cd60d-b69e-4f61-a5fc-94927d680813",
   "metadata": {},
   "source": [
    "#### Set config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c921068a-86c5-44c1-bfcd-7f37860a3c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# max_length = 384\n",
    "max_input_length = 364\n",
    "max_target_length = 32\n",
    "MAX_PREPROCESSING_LENGTH = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8943001e-f778-4451-9e25-a0f7b0219a38",
   "metadata": {},
   "source": [
    "#### Define utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a96d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    # take a batch of texts\n",
    "    text = examples[\"text\"]  # this has n_rows which = batch_size\n",
    "    # encode text\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=MAX_PREPROCESSING_LENGTH, return_tensors='pt')\n",
    "    # add labels\n",
    "    labels_batch = torch.tensor(examples['label'])\n",
    "    # create numpy array of shape (batch_size, num_labels)\n",
    "    labels_matrix = torch.nn.functional.one_hot(labels_batch)\n",
    "    labels_matrix = labels_matrix.float() \n",
    "    encoding[\"label\"] = labels_matrix#.tolist()\n",
    "  \n",
    "    return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2e68ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(ds, tokenizer, create_validation_split=False, validation_prop=0.1):\n",
    "    encoded_dataset = ds.map(preprocess_data, batched=True)\n",
    "    encoded_dataset.set_format(\"torch\")\n",
    "    \n",
    "    # create train/valid sets\n",
    "    print(f\"creating validation split: {str(create_validation_split)}\")\n",
    "    if create_validation_split:\n",
    "        train_dataset, validation_dataset= encoded_dataset['train'].train_test_split(test_size=validation_prop).values()\n",
    "    return {'train': train_dataset, 'valid': validation_dataset, 'test': encoded_dataset['test']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "138b2667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def describe_label_distr_from_data_loader(dl):\n",
    "    label_list = []\n",
    "    print(f\"#samples in split: {len(dl)}\")\n",
    "    for idx, item in enumerate(dl):\n",
    "        batch_labels_onehot = item['label']#.tolist()\n",
    "        batch_labels = [torch.argmax(label_onehot).item() for label_onehot in batch_labels_onehot]\n",
    "        label_list.extend(batch_labels)\n",
    "    print(f'distribution of labels: {collections.Counter(label_list)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff2198-508e-4496-af5b-0923b1f18d48",
   "metadata": {},
   "source": [
    "#### tokenize, encode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84eb9d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(GPT)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right' \n",
    "\n",
    "# more efficient to dynamically pad sentences to max length in a batch during collation than global max length\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7e7e34d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/llm_peft_exploration/notebooks/datasets/.cache/huggingface_datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-013e24b39d800e64.arrow\n",
      "Loading cached processed dataset at /home/jovyan/llm_peft_exploration/notebooks/datasets/.cache/huggingface_datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-dea2583dcfbada18.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating validation split: True\n",
      "distribution of labels: Counter({2: 3036, 0: 3019, 3: 3014, 1: 2931})\n",
      "distribution of labels: Counter({1: 27069, 3: 26986, 0: 26981, 2: 26964})\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = prepare_dataset(raw_dataset, tokenizer, create_validation_split=True)\n",
    "valid_loader = DataLoader(encoded_dataset['valid'], batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader = DataLoader(encoded_dataset['train'], batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "describe_label_distr_from_data_loader(valid_loader)\n",
    "describe_label_distr_from_data_loader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360db84e-f0ac-41d5-9f1d-9c3ee9b42d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebooks/openprompt_ag_news.ipynb\n",
    "notebooks/openprompt_glue_clf.ipynb\n",
    "        deleted:    notebooks/question_answering-generative.ipynb\n",
    "        deleted:    notebooks/question_answering.ipynb\n",
    "        deleted:    notebooks/sequential_training_clf_ag_news-Copy1.ipynb\n",
    "        deleted:    notebooks/vanilla_clf_ag_news-archive.ipynb\n",
    "        deleted:    notebooks/vanilla_clf_ag_news.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3491f1",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38f02c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SIZE = 0.1\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "METRIC_NAME = \"f1\"\n",
    "N_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2d43820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = Path(f\"data/models_20230606\")\n",
    "MODEL_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f2af02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(GPT, \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(label2id),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e1b1c8-b46d-454b-bcfc-9a0091c4de83",
   "metadata": {},
   "source": [
    "evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab032677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# consider std text clf that assumes more of a softmax process\n",
    "def compute_metrics_multiclass_singlelabel(p: EvalPrediction):\n",
    "    predictions = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    labels = p.label_ids\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3fccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcdd384",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Custom batch sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f90414a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = 10\n",
    "\n",
    "VALID_SIZE = 0.1\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "METRIC_NAME = \"f1\"\n",
    "N_EPOCHS = 50\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12515f4c",
   "metadata": {},
   "source": [
    "### experiment - sequential supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5079f9fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number #0:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Batch number #1:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "from training_utils import SequentialTrainingBatchSampler\n",
    "BASE=10\n",
    "custom_sequential_sampler = SequentialTrainingBatchSampler(encoded_dataset['train'], batch_size=-1, base=BASE)\n",
    "for i, batch in enumerate(custom_sequential_sampler):\n",
    "    print(f\"Batch number #{i}:  {batch}\")\n",
    "    if i>=1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d64fb9e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), 6, PosixPath('data/models_20230606'))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device, len(custom_sequential_sampler), MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e955a6-4340-442b-90e5-2fb425fba15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "total_subset_idx = []\n",
    "sequential_supervision_val_scores = []\n",
    "sequential_supervision_test_scores = []\n",
    "for idx, idx_batch in enumerate(custom_sequential_sampler):\n",
    "    #if idx<4:\n",
    "    batch_dataset = Subset(encoded_dataset['train'], idx_batch)\n",
    "    print(f\"Number of training data points: {len(idx_batch)}\")\n",
    "    args = TrainingArguments(\n",
    "        output_dir=str(MODEL_DIR)+f\"{len(idx_batch)}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    #metric_for_best_model=METRIC_NAME,\n",
    "    logging_dir='./logs',            # directory for storing logs*\n",
    "    logging_steps=2000,\n",
    "    report_to='wandb',\n",
    "    save_total_limit = 2,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "                model,\n",
    "                args,\n",
    "                train_dataset=batch_dataset,\n",
    "                eval_dataset=encoded_dataset['valid'],\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator\n",
    "            )\n",
    "    trainer.train()\n",
    "    \n",
    "    print(f\"evaluating on validation set\")\n",
    "    trainer.eval_dataset=encoded_dataset['valid']\n",
    "    val_scores = trainer.evaluate()        \n",
    "    sequential_supervision_val_scores.append(val_scores)\n",
    "    \n",
    "    print(f\"evaluating on test set\")\n",
    "    #prep\n",
    "\n",
    "\n",
    "    #trainer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27e28b85-b613-4e96-adb0-767397248c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from typing import Callable\n",
    "from transformers import EarlyStoppingCallback\n",
    "MODEL_DIR_NEW = Path(f\"data/models_20230606_new\")\n",
    "\n",
    "def train_model_series_with_sequential_sampler(batch_size=BATCH_SIZE, n_epochs=5,\n",
    "                                              compute_metrics: Callable = None, output_model_dir=MODEL_DIR_NEW, \n",
    "                                               learning_rate=LEARNING_RATE, metric_name=METRIC_NAME):\n",
    "    total_subset_idx = []\n",
    "    sequential_supervision_val_scores = []\n",
    "    sequential_supervision_test_scores = []\n",
    "    for idx, idx_batch in enumerate(custom_sequential_sampler):\n",
    "        print(idx)\n",
    "        if idx>4:\n",
    "            break\n",
    "        sequence_n_output_dir = output_model_dir.joinpath(f\"_{idx}\")\n",
    "        batch_dataset = Subset(encoded_dataset['train'], idx_batch)\n",
    "        print(f\"Number of training data points: {len(idx_batch)}\")\n",
    "        args = TrainingArguments(\n",
    "            output_dir=sequence_n_output_dir,\n",
    "            evaluation_strategy=\"steps\", #epoch\",\n",
    "            eval_steps = 500,\n",
    "            save_strategy=\"steps\", #epoch\",\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=n_epochs,\n",
    "            weight_decay=0.01,\n",
    "            metric_for_best_model=metric_name,\n",
    "            logging_dir='./logs',            # directory for storing logs*\n",
    "            logging_steps=2000,\n",
    "            # report_to='wandb',\n",
    "            load_best_model_at_end = True,\n",
    "            save_total_limit = 2,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "                    model,\n",
    "                    args,\n",
    "                    train_dataset=batch_dataset,\n",
    "                    eval_dataset=encoded_dataset['valid'],\n",
    "                    tokenizer=tokenizer,\n",
    "                    compute_metrics=compute_metrics,\n",
    "            #callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "                    #data_collator=data_collator\n",
    "                )\n",
    "        trainer.train()\n",
    "\n",
    "\n",
    "        print(f\"evaluating on test set\")\n",
    "        trainer.eval_dataset=encoded_dataset['test']\n",
    "        test_scores = trainer.evaluate()\n",
    "        sequential_supervision_test_scores.append(test_scores)\n",
    "        #trainer.save()\n",
    "\n",
    "\n",
    "        print(f\"evaluating on validation set\")\n",
    "        trainer.eval_dataset=encoded_dataset['valid']\n",
    "        val_scores = trainer.evaluate()        \n",
    "        sequential_supervision_val_scores.append(val_scores)\n",
    "    \n",
    "    return sequential_supervision_val_scores, sequential_supervision_test_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "492efa72-1ea1-4c46-970c-63c24bbe8013",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/jovyan/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Number of training data points: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='613' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 23:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on validation set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/jovyan/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 20\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Number of training data points: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 02:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='613' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 23:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on validation set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/jovyan/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 160\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Number of training data points: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 20:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='613' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 23:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on validation set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/jovyan/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Number of training data points: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1565' max='1565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1565/1565 4:05:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.118460</td>\n",
       "      <td>0.912121</td>\n",
       "      <td>0.938667</td>\n",
       "      <td>0.893500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.119375</td>\n",
       "      <td>0.918987</td>\n",
       "      <td>0.945083</td>\n",
       "      <td>0.904500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.121148</td>\n",
       "      <td>0.920249</td>\n",
       "      <td>0.946097</td>\n",
       "      <td>0.906750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_3/checkpoint-500\n",
      "Configuration saved in data/models_20230606_new/_3/checkpoint-500/config.json\n",
      "Model weights saved in data/models_20230606_new/_3/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_3/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_3/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_3/checkpoint-1000\n",
      "Configuration saved in data/models_20230606_new/_3/checkpoint-1000/config.json\n",
      "Model weights saved in data/models_20230606_new/_3/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_3/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_3/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_3/checkpoint-1500\n",
      "Configuration saved in data/models_20230606_new/_3/checkpoint-1500/config.json\n",
      "Model weights saved in data/models_20230606_new/_3/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_3/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_3/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_3/checkpoint-500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from data/models_20230606_new/_3/checkpoint-1500 (score: 0.9202489869240088).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='613' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 23:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on validation set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/jovyan/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 100000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15625\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Number of training data points: 100000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15625' max='15625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15625/15625 41:01:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.101411</td>\n",
       "      <td>0.925276</td>\n",
       "      <td>0.949333</td>\n",
       "      <td>0.913167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.095647</td>\n",
       "      <td>0.929044</td>\n",
       "      <td>0.951528</td>\n",
       "      <td>0.917917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.093947</td>\n",
       "      <td>0.931352</td>\n",
       "      <td>0.952917</td>\n",
       "      <td>0.921083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>0.088588</td>\n",
       "      <td>0.934282</td>\n",
       "      <td>0.954861</td>\n",
       "      <td>0.925083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>0.090575</td>\n",
       "      <td>0.933149</td>\n",
       "      <td>0.954639</td>\n",
       "      <td>0.924083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>0.083051</td>\n",
       "      <td>0.938470</td>\n",
       "      <td>0.957847</td>\n",
       "      <td>0.930500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>0.087891</td>\n",
       "      <td>0.938542</td>\n",
       "      <td>0.958236</td>\n",
       "      <td>0.931500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.093764</td>\n",
       "      <td>0.937957</td>\n",
       "      <td>0.957847</td>\n",
       "      <td>0.931917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.085054</td>\n",
       "      <td>0.936865</td>\n",
       "      <td>0.956708</td>\n",
       "      <td>0.929417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.082930</td>\n",
       "      <td>0.940764</td>\n",
       "      <td>0.959750</td>\n",
       "      <td>0.932667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.089090</td>\n",
       "      <td>0.937343</td>\n",
       "      <td>0.957500</td>\n",
       "      <td>0.930917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>0.083280</td>\n",
       "      <td>0.941486</td>\n",
       "      <td>0.960500</td>\n",
       "      <td>0.934583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>0.086314</td>\n",
       "      <td>0.937610</td>\n",
       "      <td>0.957750</td>\n",
       "      <td>0.930417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>0.085367</td>\n",
       "      <td>0.940872</td>\n",
       "      <td>0.959736</td>\n",
       "      <td>0.934250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>0.086309</td>\n",
       "      <td>0.941893</td>\n",
       "      <td>0.960722</td>\n",
       "      <td>0.935333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.081899</td>\n",
       "      <td>0.943089</td>\n",
       "      <td>0.961222</td>\n",
       "      <td>0.936667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.081116</td>\n",
       "      <td>0.943458</td>\n",
       "      <td>0.961431</td>\n",
       "      <td>0.938083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.083100</td>\n",
       "      <td>0.943362</td>\n",
       "      <td>0.961736</td>\n",
       "      <td>0.938333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.084524</td>\n",
       "      <td>0.944437</td>\n",
       "      <td>0.962403</td>\n",
       "      <td>0.938833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.087843</td>\n",
       "      <td>0.944356</td>\n",
       "      <td>0.962139</td>\n",
       "      <td>0.938417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.084755</td>\n",
       "      <td>0.939746</td>\n",
       "      <td>0.959000</td>\n",
       "      <td>0.933583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.091364</td>\n",
       "      <td>0.941039</td>\n",
       "      <td>0.960472</td>\n",
       "      <td>0.936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.085915</td>\n",
       "      <td>0.942400</td>\n",
       "      <td>0.960875</td>\n",
       "      <td>0.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.089408</td>\n",
       "      <td>0.940198</td>\n",
       "      <td>0.959264</td>\n",
       "      <td>0.934833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.088916</td>\n",
       "      <td>0.940533</td>\n",
       "      <td>0.959694</td>\n",
       "      <td>0.934583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.094656</td>\n",
       "      <td>0.938646</td>\n",
       "      <td>0.958403</td>\n",
       "      <td>0.932833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.090694</td>\n",
       "      <td>0.941275</td>\n",
       "      <td>0.960347</td>\n",
       "      <td>0.935583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.093714</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.960111</td>\n",
       "      <td>0.935167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.089563</td>\n",
       "      <td>0.941127</td>\n",
       "      <td>0.960139</td>\n",
       "      <td>0.935167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.091335</td>\n",
       "      <td>0.940695</td>\n",
       "      <td>0.959986</td>\n",
       "      <td>0.935333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.091330</td>\n",
       "      <td>0.940749</td>\n",
       "      <td>0.960083</td>\n",
       "      <td>0.935583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-1000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-1000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-1500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-1500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-2000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-2000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-2500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-2500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-3000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-3000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-3500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-3500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-2500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-4000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-4000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-4500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-4500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-4000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-5000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-5000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-3500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-5500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-5500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-4500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-6000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-6000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-5000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-6500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-6500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-5500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-7000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-7000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-6500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-7500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-7500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-6000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-8000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-8000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-7000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-8500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-8500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-7500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-9000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-9000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-8000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-9500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-9500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-8500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-10000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-10000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-9000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-10500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-10500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-10000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-14000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-14000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-13500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-14500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-14500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-14000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-15000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-15000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-14500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-15500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-15500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-15000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from data/models_20230606_new/_4/checkpoint-9500 (score: 0.9444374869547068).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='613' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 23:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on validation set\n",
      "5\n",
      "[0.10525, 0.171, 0.8311666666666667, 0.90675, 0.9388333333333333]\n",
      "[0.0968421052631579, 0.16486842105263158, 0.8302631578947368, 0.910921052631579, 0.9415789473684211]\n"
     ]
    }
   ],
   "source": [
    "sequential_supervision_val_scores, sequential_supervision_test_scores = train_model_series_with_sequential_sampler(compute_metrics=compute_metrics)\n",
    "print([score['eval_accuracy'] for score in sequential_supervision_val_scores])\n",
    "print([score['eval_accuracy'] for score in sequential_supervision_test_scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6a4e93-2303-4987-aea0-1f6b105c5825",
   "metadata": {},
   "source": [
    "#### writing to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594fa72d-3928-4583-84d2-bf84fda560f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(MODEL_DIR_NEW.joinpath(\"val_scores_sequential_vanilla_clf.json\"), \"w+\") as f:\n",
    "    json.dump(sequential_supervision_val_scores, f)\n",
    "with open(MODEL_DIR_NEW.joinpath(\"test_scores_sequential_vanilla_clf.json\"), \"w+\") as f:\n",
    "    json.dump(sequential_supervision_test_scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5312284d-8824-47fd-a63c-f49b22f01cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[0.0968421052631579, 0.16486842105263158, 0.8302631578947368, 0.910921052631579, 0.9415789473684211]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b2b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "0,00\t0.0006\n",
    "0.3325\t0.0011\n",
    "0.4955\t0.0166\n",
    "0.6229\t0.2606\n",
    "0.7095\t0.8776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39da19dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "   'GPT2 generation tuned with task-specific prompt': [0.0000, 0.3325, 0.4955, 0.6229, 0.7095],\n",
    "   'GPT2 adapted with a Classifier head': [0.0006, 0.0011, 0.0166, 0.2606, 0.8776],\n",
    "}, index=[10, 100, 1000, 10000, 100000])\n",
    "lines = df.plot.line(style=\".-\")\n",
    "lines.set_ylabel(\"accuracy\")\n",
    "lines.set_xlabel(\"# training examples\")\n",
    "lines.set_title(\"AG-News Topic Classification performance on test set\")\n",
    "fig = lines.get_figure()\n",
    "fig.savefig(\"data_points_vs_prompts_ag_news.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0655e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-bioblp-env [Python]",
   "language": "python",
   "name": "conda-env-.conda-bioblp-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
