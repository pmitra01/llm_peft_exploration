{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c53c27ca",
   "metadata": {},
   "source": [
    "# Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "517e2e60-a8ac-43d0-af93-d6f6fdd9c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebef213",
   "metadata": {},
   "source": [
    "### imports and globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d956186",
   "metadata": {},
   "outputs": [],
   "source": [
    "T5_SMALL = \"t5-small\"\n",
    "GPT = \"gpt2\"  # 117M parameters as per https://huggingface.co/transformers/v3.3.1/pretrained_models.html # \"openai-gpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71800546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt\n",
    "from collections import defaultdict\n",
    "\n",
    "# mandatory imports\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.data import random_split\n",
    "import collections\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baeb44ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98a779",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae590d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SIZE = 0.1\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "METRIC_NAME = \"f1\"\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a90ce66",
   "metadata": {
    "tags": []
   },
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c580ec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ag_news (/home/jovyan/llm_peft_exploration/notebooks/datasets/.cache/huggingface_datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841ae0fdb3f64f8fbf714a891afa7adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset = load_dataset('ag_news', cache_dir=\"./datasets/.cache/huggingface_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f67202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\"\n",
    "}\n",
    "\n",
    "label2id = {v:k for k,v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87072172",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### explore shape of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0669ab23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 120000, 'test': 7600}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: len(raw_dataset[k]) for k in raw_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57d45cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\",\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['test'][0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc660581",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocess dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249cd60d-b69e-4f61-a5fc-94927d680813",
   "metadata": {},
   "source": [
    "#### Set config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c921068a-86c5-44c1-bfcd-7f37860a3c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# max_length = 384\n",
    "max_input_length = 364\n",
    "max_target_length = 32\n",
    "MAX_PREPROCESSING_LENGTH = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8943001e-f778-4451-9e25-a0f7b0219a38",
   "metadata": {},
   "source": [
    "#### Define utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a96d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    # take a batch of texts\n",
    "    text = examples[\"text\"]  # this has n_rows which = batch_size\n",
    "    # encode text\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=MAX_PREPROCESSING_LENGTH, return_tensors='pt')\n",
    "    # add labels\n",
    "    labels_batch = torch.tensor(examples['label'])\n",
    "    # create numpy array of shape (batch_size, num_labels)\n",
    "    labels_matrix = torch.nn.functional.one_hot(labels_batch)\n",
    "    labels_matrix = labels_matrix.float() \n",
    "    encoding[\"label\"] = labels_matrix#.tolist()\n",
    "  \n",
    "    return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2e68ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(ds, tokenizer, create_validation_split=False, validation_prop=0.1):\n",
    "    encoded_dataset = ds.map(preprocess_data, batched=True)\n",
    "    encoded_dataset.set_format(\"torch\")\n",
    "    \n",
    "    # create train/valid sets\n",
    "    print(f\"creating validation split: {str(create_validation_split)}\")\n",
    "    if create_validation_split:\n",
    "        train_dataset, validation_dataset= encoded_dataset['train'].train_test_split(test_size=validation_prop).values()\n",
    "    return {'train': train_dataset, 'valid': validation_dataset, 'test': encoded_dataset['test']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "138b2667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def describe_label_distr_from_data_loader(dl):\n",
    "    label_list = []\n",
    "    print(f\"#samples in split: {len(dl)}\")\n",
    "    for idx, item in enumerate(dl):\n",
    "        batch_labels_onehot = item['label']#.tolist()\n",
    "        batch_labels = [torch.argmax(label_onehot).item() for label_onehot in batch_labels_onehot]\n",
    "        label_list.extend(batch_labels)\n",
    "    print(f'distribution of labels: {collections.Counter(label_list)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff2198-508e-4496-af5b-0923b1f18d48",
   "metadata": {},
   "source": [
    "#### tokenize, encode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84eb9d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(GPT)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right' \n",
    "\n",
    "# more efficient to dynamically pad sentences to max length in a batch during collation than global max length\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7e7e34d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/llm_peft_exploration/notebooks/datasets/.cache/huggingface_datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-013e24b39d800e64.arrow\n",
      "Loading cached processed dataset at /home/jovyan/llm_peft_exploration/notebooks/datasets/.cache/huggingface_datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-dea2583dcfbada18.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating validation split: True\n",
      "distribution of labels: Counter({2: 3036, 0: 3019, 3: 3014, 1: 2931})\n",
      "distribution of labels: Counter({1: 27069, 3: 26986, 0: 26981, 2: 26964})\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = prepare_dataset(raw_dataset, tokenizer, create_validation_split=True)\n",
    "valid_loader = DataLoader(encoded_dataset['valid'], batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader = DataLoader(encoded_dataset['train'], batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "describe_label_distr_from_data_loader(valid_loader)\n",
    "describe_label_distr_from_data_loader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360db84e-f0ac-41d5-9f1d-9c3ee9b42d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebooks/openprompt_ag_news.ipynb\n",
    "notebooks/openprompt_glue_clf.ipynb\n",
    "notebooks/question_answering-generative.ipynb\n",
    "notebooks/question_answering.ipynb\n",
    "notebooks/sequential_training_clf_ag_news-Copy1.ipynb\n",
    "notebooks/vanilla_clf_ag_news-archive.ipynb\n",
    "notebooks/vanilla_clf_ag_news.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3491f1",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38f02c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SIZE = 0.1\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "METRIC_NAME = \"f1\"\n",
    "N_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2d43820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = Path(f\"data/models_20230606\")\n",
    "MODEL_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f2af02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(GPT, \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(label2id),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e1b1c8-b46d-454b-bcfc-9a0091c4de83",
   "metadata": {},
   "source": [
    "evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab032677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# consider std text clf that assumes more of a softmax process\n",
    "def compute_metrics_multiclass_singlelabel(p: EvalPrediction):\n",
    "    predictions = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    labels = p.label_ids\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3fccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcdd384",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Custom batch sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f90414a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = 10\n",
    "\n",
    "VALID_SIZE = 0.1\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "METRIC_NAME = \"f1\"\n",
    "N_EPOCHS = 50\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12515f4c",
   "metadata": {},
   "source": [
    "### experiment - sequential supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5079f9fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number #0:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Batch number #1:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "from training_utils import SequentialTrainingBatchSampler\n",
    "custom_sequential_sampler = SequentialTrainingBatchSampler(encoded_dataset['train'], batch_size=-1, base=BASE)\n",
    "for i, batch in enumerate(custom_sequential_sampler):\n",
    "    print(f\"Batch number #{i}:  {batch}\")\n",
    "    if i>=1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d64fb9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), 6, PosixPath('data/models_20230606'))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device, len(custom_sequential_sampler), MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27e28b85-b613-4e96-adb0-767397248c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from typing import Callable\n",
    "from transformers import EarlyStoppingCallback\n",
    "MODEL_DIR_NEW = Path(f\"data/models_20230606_new\")\n",
    "\n",
    "def train_model_series_with_sequential_sampler(batch_size=BATCH_SIZE, n_epochs=5,\n",
    "                                              compute_metrics: Callable = None, output_model_dir=MODEL_DIR_NEW, \n",
    "                                               learning_rate=LEARNING_RATE, metric_name=METRIC_NAME):\n",
    "    total_subset_idx = []\n",
    "    sequential_supervision_val_scores = []\n",
    "    sequential_supervision_test_scores = []\n",
    "    for idx, idx_batch in enumerate(custom_sequential_sampler):\n",
    "        print(idx)\n",
    "        if idx>4:\n",
    "            break\n",
    "        sequence_n_output_dir = output_model_dir.joinpath(f\"_{idx}\")\n",
    "        batch_dataset = Subset(encoded_dataset['train'], idx_batch)\n",
    "        print(f\"Number of training data points: {len(idx_batch)}\")\n",
    "        args = TrainingArguments(\n",
    "            output_dir=sequence_n_output_dir,\n",
    "            evaluation_strategy=\"steps\", #epoch\",\n",
    "            eval_steps = 500,\n",
    "            save_strategy=\"steps\", #epoch\",\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=n_epochs,\n",
    "            weight_decay=0.01,\n",
    "            metric_for_best_model=metric_name,\n",
    "            logging_dir='./logs',            # directory for storing logs*\n",
    "            logging_steps=2000,\n",
    "            # report_to='wandb',\n",
    "            load_best_model_at_end = True,\n",
    "            save_total_limit = 2,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "                    model,\n",
    "                    args,\n",
    "                    train_dataset=batch_dataset,\n",
    "                    eval_dataset=encoded_dataset['valid'],\n",
    "                    tokenizer=tokenizer,\n",
    "                    compute_metrics=compute_metrics,\n",
    "            #callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "                    #data_collator=data_collator\n",
    "                )\n",
    "        trainer.train()\n",
    "\n",
    "\n",
    "        print(f\"evaluating on test set\")\n",
    "        trainer.eval_dataset=encoded_dataset['test']\n",
    "        test_scores = trainer.evaluate()\n",
    "        sequential_supervision_test_scores.append(test_scores)\n",
    "        #trainer.save()\n",
    "\n",
    "\n",
    "        print(f\"evaluating on validation set\")\n",
    "        trainer.eval_dataset=encoded_dataset['valid']\n",
    "        val_scores = trainer.evaluate()        \n",
    "        sequential_supervision_val_scores.append(val_scores)\n",
    "    \n",
    "    return sequential_supervision_val_scores, sequential_supervision_test_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "492efa72-1ea1-4c46-970c-63c24bbe8013",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/jovyan/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Number of training data points: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='613' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 23:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on validation set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/jovyan/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 20\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Number of training data points: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 02:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='613' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 23:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on validation set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/jovyan/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 160\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Number of training data points: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 20:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='613' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 23:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on validation set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/jovyan/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Number of training data points: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1565' max='1565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1565/1565 4:05:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.118460</td>\n",
       "      <td>0.912121</td>\n",
       "      <td>0.938667</td>\n",
       "      <td>0.893500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.119375</td>\n",
       "      <td>0.918987</td>\n",
       "      <td>0.945083</td>\n",
       "      <td>0.904500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.121148</td>\n",
       "      <td>0.920249</td>\n",
       "      <td>0.946097</td>\n",
       "      <td>0.906750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_3/checkpoint-500\n",
      "Configuration saved in data/models_20230606_new/_3/checkpoint-500/config.json\n",
      "Model weights saved in data/models_20230606_new/_3/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_3/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_3/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_3/checkpoint-1000\n",
      "Configuration saved in data/models_20230606_new/_3/checkpoint-1000/config.json\n",
      "Model weights saved in data/models_20230606_new/_3/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_3/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_3/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_3/checkpoint-1500\n",
      "Configuration saved in data/models_20230606_new/_3/checkpoint-1500/config.json\n",
      "Model weights saved in data/models_20230606_new/_3/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_3/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_3/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_3/checkpoint-500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from data/models_20230606_new/_3/checkpoint-1500 (score: 0.9202489869240088).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='613' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 23:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on validation set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/jovyan/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 100000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15625\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Number of training data points: 100000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15625' max='15625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15625/15625 41:01:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.101411</td>\n",
       "      <td>0.925276</td>\n",
       "      <td>0.949333</td>\n",
       "      <td>0.913167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.095647</td>\n",
       "      <td>0.929044</td>\n",
       "      <td>0.951528</td>\n",
       "      <td>0.917917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.093947</td>\n",
       "      <td>0.931352</td>\n",
       "      <td>0.952917</td>\n",
       "      <td>0.921083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>0.088588</td>\n",
       "      <td>0.934282</td>\n",
       "      <td>0.954861</td>\n",
       "      <td>0.925083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>0.090575</td>\n",
       "      <td>0.933149</td>\n",
       "      <td>0.954639</td>\n",
       "      <td>0.924083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>0.083051</td>\n",
       "      <td>0.938470</td>\n",
       "      <td>0.957847</td>\n",
       "      <td>0.930500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>0.087891</td>\n",
       "      <td>0.938542</td>\n",
       "      <td>0.958236</td>\n",
       "      <td>0.931500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.093764</td>\n",
       "      <td>0.937957</td>\n",
       "      <td>0.957847</td>\n",
       "      <td>0.931917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.085054</td>\n",
       "      <td>0.936865</td>\n",
       "      <td>0.956708</td>\n",
       "      <td>0.929417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.082930</td>\n",
       "      <td>0.940764</td>\n",
       "      <td>0.959750</td>\n",
       "      <td>0.932667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.089090</td>\n",
       "      <td>0.937343</td>\n",
       "      <td>0.957500</td>\n",
       "      <td>0.930917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>0.083280</td>\n",
       "      <td>0.941486</td>\n",
       "      <td>0.960500</td>\n",
       "      <td>0.934583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>0.086314</td>\n",
       "      <td>0.937610</td>\n",
       "      <td>0.957750</td>\n",
       "      <td>0.930417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>0.085367</td>\n",
       "      <td>0.940872</td>\n",
       "      <td>0.959736</td>\n",
       "      <td>0.934250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>0.086309</td>\n",
       "      <td>0.941893</td>\n",
       "      <td>0.960722</td>\n",
       "      <td>0.935333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.081899</td>\n",
       "      <td>0.943089</td>\n",
       "      <td>0.961222</td>\n",
       "      <td>0.936667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.081116</td>\n",
       "      <td>0.943458</td>\n",
       "      <td>0.961431</td>\n",
       "      <td>0.938083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.083100</td>\n",
       "      <td>0.943362</td>\n",
       "      <td>0.961736</td>\n",
       "      <td>0.938333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.084524</td>\n",
       "      <td>0.944437</td>\n",
       "      <td>0.962403</td>\n",
       "      <td>0.938833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.087843</td>\n",
       "      <td>0.944356</td>\n",
       "      <td>0.962139</td>\n",
       "      <td>0.938417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.084755</td>\n",
       "      <td>0.939746</td>\n",
       "      <td>0.959000</td>\n",
       "      <td>0.933583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.091364</td>\n",
       "      <td>0.941039</td>\n",
       "      <td>0.960472</td>\n",
       "      <td>0.936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.085915</td>\n",
       "      <td>0.942400</td>\n",
       "      <td>0.960875</td>\n",
       "      <td>0.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.089408</td>\n",
       "      <td>0.940198</td>\n",
       "      <td>0.959264</td>\n",
       "      <td>0.934833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.088916</td>\n",
       "      <td>0.940533</td>\n",
       "      <td>0.959694</td>\n",
       "      <td>0.934583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.094656</td>\n",
       "      <td>0.938646</td>\n",
       "      <td>0.958403</td>\n",
       "      <td>0.932833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.090694</td>\n",
       "      <td>0.941275</td>\n",
       "      <td>0.960347</td>\n",
       "      <td>0.935583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.093714</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.960111</td>\n",
       "      <td>0.935167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.089563</td>\n",
       "      <td>0.941127</td>\n",
       "      <td>0.960139</td>\n",
       "      <td>0.935167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.091335</td>\n",
       "      <td>0.940695</td>\n",
       "      <td>0.959986</td>\n",
       "      <td>0.935333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.091330</td>\n",
       "      <td>0.940749</td>\n",
       "      <td>0.960083</td>\n",
       "      <td>0.935583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-1000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-1000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-1500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-1500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-2000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-2000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-2500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-2500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-3000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-3000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-3500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-3500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-2500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-4000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-4000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-4500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-4500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-4000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-5000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-5000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-3500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-5500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-5500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-4500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-6000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-6000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-5000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-6500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-6500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-5500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-7000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-7000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-6500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-7500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-7500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-6000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-8000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-8000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-7000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-8500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-8500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-7500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-9000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-9000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-8000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-9500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-9500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-8500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-10000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-10000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-9000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-10500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-10500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-10000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-14000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-14000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-13500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-14500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-14500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-14000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-15000\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-15000/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-14500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to data/models_20230606_new/_4/checkpoint-15500\n",
      "Configuration saved in data/models_20230606_new/_4/checkpoint-15500/config.json\n",
      "Model weights saved in data/models_20230606_new/_4/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in data/models_20230606_new/_4/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in data/models_20230606_new/_4/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [data/models_20230606_new/_4/checkpoint-15000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from data/models_20230606_new/_4/checkpoint-9500 (score: 0.9444374869547068).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='613' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 23:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on validation set\n",
      "5\n",
      "[0.10525, 0.171, 0.8311666666666667, 0.90675, 0.9388333333333333]\n",
      "[0.0968421052631579, 0.16486842105263158, 0.8302631578947368, 0.910921052631579, 0.9415789473684211]\n"
     ]
    }
   ],
   "source": [
    "sequential_supervision_val_scores, sequential_supervision_test_scores = train_model_series_with_sequential_sampler(compute_metrics=compute_metrics)\n",
    "print([score['eval_accuracy'] for score in sequential_supervision_val_scores])\n",
    "print([score['eval_accuracy'] for score in sequential_supervision_test_scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6a4e93-2303-4987-aea0-1f6b105c5825",
   "metadata": {},
   "source": [
    "#### writing to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594fa72d-3928-4583-84d2-bf84fda560f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(MODEL_DIR_NEW.joinpath(\"val_scores_sequential_vanilla_clf.json\"), \"w+\") as f:\n",
    "    json.dump(sequential_supervision_val_scores, f)\n",
    "with open(MODEL_DIR_NEW.joinpath(\"test_scores_sequential_vanilla_clf.json\"), \"w+\") as f:\n",
    "    json.dump(sequential_supervision_test_scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5312284d-8824-47fd-a63c-f49b22f01cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[0.0968421052631579, 0.16486842105263158, 0.8302631578947368, 0.910921052631579, 0.9415789473684211]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b2b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "0,00\t0.0006\n",
    "0.3325\t0.0011\n",
    "0.4955\t0.0166\n",
    "0.6229\t0.2606\n",
    "0.7095\t0.8776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39da19dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "   'GPT2 generation tuned with task-specific prompt': [0.0000, 0.3325, 0.4955, 0.6229, 0.7095],\n",
    "   'GPT2 adapted with a Classifier head': [0.0006, 0.0011, 0.0166, 0.2606, 0.8776],\n",
    "}, index=[10, 100, 1000, 10000, 100000])\n",
    "lines = df.plot.line(style=\"o-\", figsize=(8,8), grid=True)\n",
    "lines.set_ylabel(\"accuracy\")\n",
    "lines.set_xlabel(\"# training examples\")\n",
    "lines.set_title(\"AG-News Topic Classification performance on test set\")\n",
    "fig = lines.get_figure()\n",
    "fig.savefig(\"data_points_vs_prompts_ag_news.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0655e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-bioblp-env [Python]",
   "language": "python",
   "name": "conda-env-.conda-bioblp-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
