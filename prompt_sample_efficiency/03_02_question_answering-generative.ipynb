{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zYUevg8M7vQ"
   },
   "source": [
    "# Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "hide_input": true,
    "id": "WeDnIX7VM7vQ",
    "outputId": "0be11939-1389-41be-e863-f1a214234eaf"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "e2iAlRcbM7vS"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e54737564184aee9fc692208f4917c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTILBERT = \"distilbert-base-uncased\"\n",
    "T5_SMALL = \"t5-small\"\n",
    "GPT2 = \"GPT2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woyYDSPDM7vS"
   },
   "source": [
    "## Load SQuAD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2OaCRYWM7vS"
   },
   "source": [
    "Start by loading a smaller subset of the SQuAD dataset from the ðŸ¤— Datasets library. This'll give you a chance to experiment and make sure everything works before spending more time training on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-XYWGZzxM7vT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/home/jovyan/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebcf725f9dc4ad88a95097f7187c568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"squad\")#, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FGy_sHcM7vT"
   },
   "source": [
    "Split the dataset's `train` split into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7hz9Bu-gM7vT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#raw_datasets = squad.train_test_split(test_size=0.2)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxutqCMyM7vT"
   },
   "source": [
    "Then take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4RUyI0MqM7vT",
    "outputId": "8f86d968-c09b-4c1c-dcc0-fae7fd3045b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhUzGuuWM7vU"
   },
   "source": [
    "There are several important fields here:\n",
    "\n",
    "- `answers`: the starting location of the answer token and the answer text.\n",
    "- `context`: background information from which the model needs to extract the answer.\n",
    "- `question`: the question a model should answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFK5jLlrM7vU"
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RX456vooM7vU"
   },
   "source": [
    "The next step is to load a DistilBERT tokenizer to process the `question` and `context` fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "W2gutx8HM7vU"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(T5_SMALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### play around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast tokenizer: True\n",
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "tokens:\t ['â–Architectural', 'ly', ',', 'â–the', 'â–school', 'â–has', 'â–', 'a', 'â–Catholic', 'â–character', '.', 'â–A', 'top', 'â–the', 'â–Main', 'â–Building', \"'\", 's', 'â–gold', 'â–dome', 'â–is', 'â–', 'a', 'â–golden', 'â–statue', 'â–of', 'â–the', 'â–Virgin', 'â–Mary', '.', 'â–', 'Immediately', 'â–in', 'â–front', 'â–of', 'â–the', 'â–Main', 'â–Building', 'â–and', 'â–facing', 'â–it', ',', 'â–is', 'â–', 'a', 'â–copper', 'â–statue', 'â–of', 'â–Christ', 'â–with', 'â–arms', 'â–', 'u', 'praised', 'â–with', 'â–the', 'â–legend', 'â–\"', 'V', 'e', 'nite', 'â–Ad', 'â–Me', 'â–Om', 'nes', '\".', 'â–Next', 'â–to', 'â–the', 'â–Main', 'â–Building', 'â–is', 'â–the', 'â–Basil', 'ica', 'â–of', 'â–the', 'â–', 'Sacred', 'â–Heart', '.', 'â–', 'Immediately', 'â–behind', 'â–the', 'â–basil', 'ica', 'â–is', 'â–the', 'â–Gro', 't', 'to', ',', 'â–', 'a', 'â–Marian', 'â–place', 'â–of', 'â–prayer', 'â–and', 'â–reflection', '.', 'â–It', 'â–is', 'â–', 'a', 'â–replica', 'â–of', 'â–the', 'â–', 'gro', 't', 'to', 'â–at', 'â–L', 'our', 'des', ',', 'â–France', 'â–where', 'â–the', 'â–Virgin', 'â–Mary', 'â–', 'repute', 'd', 'ly', 'â–appeared', 'â–to', 'â–Saint', 'â–Bern', 'a', 'd', 'ette', 'â–So', 'ubi', 'rous', 'â–in', 'â–18', '58', '.', 'â–At', 'â–the', 'â–end', 'â–of', 'â–the', 'â–main', 'â–drive', 'â–(', 'and', 'â–in', 'â–', 'a', 'â–direct', 'â–line', 'â–that', 'â–connect', 's', 'â–through', 'â–3', 'â–statue', 's', 'â–and', 'â–the', 'â–Gold', 'â–Dom', 'e', '),', 'â–is', 'â–', 'a', 'â–simple', ',', 'â–modern', 'â–stone', 'â–statue', 'â–of', 'â–Mary', '.', '</s>']\n",
      "word_ids that tokens correspond to:\t [0, 0, 0, 1, 2, 3, 4, 4, 5, 6, 6, 7, 7, 8, 9, 10, 10, 10, 11, 12, 13, 14, 14, 15, 16, 17, 18, 19, 20, 20, 21, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 30, 31, 32, 32, 33, 34, 35, 36, 37, 38, 39, 39, 39, 40, 41, 42, 43, 43, 43, 43, 44, 45, 46, 46, 46, 47, 48, 49, 50, 51, 52, 53, 54, 54, 55, 56, 57, 57, 58, 58, 59, 59, 60, 61, 62, 62, 63, 64, 65, 65, 65, 65, 66, 66, 67, 68, 69, 70, 71, 72, 72, 73, 74, 75, 75, 76, 77, 78, 79, 79, 79, 79, 80, 81, 81, 81, 81, 82, 83, 84, 85, 86, 87, 87, 87, 87, 88, 89, 90, 91, 91, 91, 91, 92, 92, 92, 93, 94, 94, 94, 95, 96, 97, 98, 99, 100, 101, 102, 102, 103, 104, 104, 105, 106, 107, 108, 108, 109, 110, 111, 111, 112, 113, 114, 115, 115, 115, 116, 117, 117, 118, 118, 119, 120, 121, 122, 123, 123, None]\n"
     ]
    }
   ],
   "source": [
    "print(f'fast tokenizer: {tokenizer.is_fast}')\n",
    "sample = raw_datasets['train'][0]\n",
    "print(sample['context'])\n",
    "encodings = tokenizer(sample['context'])\n",
    "tokenizer.decode(encodings['input_ids'])\n",
    "print('tokens:\\t', encodings.tokens())\n",
    "print('word_ids that tokens correspond to:\\t', encodings.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?</s> Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart</s>\n",
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?</s> Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the </s>\n",
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?</s>nite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858</s>\n",
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?</s>tto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is </s>\n",
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?</s> the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</s>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    sample['question'],\n",
    "    sample['context'],\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    ")\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**overflow_to_sample_mapping**\n",
    "\n",
    "As we can see, we get back the usual input IDs, token type IDs, and attention mask, as well as the offset mapping we required and an extra key, `overflow_to_sample_mapping`. The corresponding value will be of use to us when we tokenize several texts at the same time (which we should do to benefit from the fact that our tokenizer is backed by Rust). Since one sample can give several features, it maps each feature to the example it originated from. Because here we only tokenized one example, we get a list of 0s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"overflow_to_sample_mapping\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " But if we tokenize more examples, this will become more useful (Seen below):\n",
    " \n",
    " As we can see, the first three examples (at indices 2, 3, and 4 in the training set) each gave four features and the last example (at index 5 in the training set) gave 7 features.\n",
    "\n",
    "This information will be useful to map each feature we get to its corresponding label. As mentioned earlier, those labels are:\n",
    "\n",
    "(0, 0) if the answer is not in the corresponding span of the context\n",
    "(start_position, end_position) if the answer is in the corresponding span of the context, with start_position being the index of the token (in the input IDs) at the start of the answer and end_position being the index of the token (in the input IDs) where the answer ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4 examples gave 21 features.\n",
      "Here is where each comes from: [0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3].\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    raw_datasets[\"train\"][2:6][\"question\"],\n",
    "    raw_datasets[\"train\"][2:6][\"context\"],\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n",
    "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['overflow_to_sample_mapping']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### back to business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6vf0wiJM7vU"
   },
   "source": [
    "There are a few preprocessing steps particular to question answering tasks you should be aware of:\n",
    "\n",
    "1. Some examples in a dataset may have a very long `context` that exceeds the maximum input length of the model. To deal with longer sequences, truncate only the `context` by setting `truncation=\"only_second\"`.\n",
    "2. Next, map the start and end positions of the answer to the original `context` by setting\n",
    "   `return_offset_mapping=True`.\n",
    "3. With the mapping in hand, now you can find the start and end tokens of the answer. Use the [sequence_ids](https://huggingface.co/docs/tokenizers/main/en/api/encoding#tokenizers.Encoding.sequence_ids) method to\n",
    "   find which part of the offset corresponds to the `question` and which corresponds to the `context`.\n",
    "\n",
    "Here is how you can create a function to truncate and map the start and end tokens of the `answer` to the `context`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the answer offsets are provided as character offsets. We transform those to token level offsets using the `preprocess_function` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'frequency count')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "question_lens = []\n",
    "context_lens = []\n",
    "for sample in raw_datasets[\"train\"][\"context\"]:\n",
    "    context_lens.append(len(sample))\n",
    "\n",
    "import matplotlib.pyplot as plt    \n",
    "plt.hist(context_lens)\n",
    "plt.xlabel('length of provided context for question')\n",
    "plt.ylabel('frequency count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "90VohFJRM7vU"
   },
   "outputs": [],
   "source": [
    "# max_length = 384\n",
    "stride = 128\n",
    "\n",
    "max_input_length = 256\n",
    "max_target_length = 64\n",
    "\n",
    "\n",
    "prefix1 = \"Given context: \"\n",
    "prefix2 = \"Answer the Question: \"\n",
    "suffix1 = \"\"  # \"Answer: \"\n",
    "\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    context = [prefix1 + doc for doc in examples[\"context\"] ]\n",
    "    questions = [prefix2 + q.strip() + suffix1 for q in examples[\"question\"]]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        context,\n",
    "        questions,\n",
    "        max_length=max_input_length,\n",
    "        truncation=\"only_first\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    answers_text = []\n",
    "    \n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        answer_text = answer[\"text\"][0]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            answers_text.append(\"\")\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "            answers_text.append(answer_text)\n",
    "            \n",
    "    labels = tokenizer(\n",
    "        answers_text, max_length=max_target_length, truncation=True\n",
    "    )\n",
    "    #print(labels)\n",
    "    #if (start_positions and end_positions):\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DOUBT: is it okay to select the answer as first instance in list diring training?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_WN8Mo9M7vU"
   },
   "source": [
    "To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) function. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once. Remove any columns you don't need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b63a101bbe4947a38ed35d7b753b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10570, 13104)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"validation\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[:1]['labels'][0])\n",
    "#len(train_dataset['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uwa8TeQTM7vV"
   },
   "source": [
    "Now create a batch of examples using [DefaultDataCollator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator). Unlike other data collators in ðŸ¤— Transformers, the [DefaultDataCollator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator) does not apply any additional preprocessing such as padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "7wACtRHVM7vV"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorForSeq2Seq\n\u001b[0;32m----> 3\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForSeq2Seq(tokenizer, model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the validation data\n",
    "Preprocessing the validation data will be slightly easier as we donâ€™t need to generate labels (unless we want to compute a validation loss, but that number wonâ€™t really help us understand how good the model is). The real joy will be to interpret the predictions of the model into spans of the original context. For this, we will just need to store both the offset mappings and some way to match each created feature to the original example it comes from. Since there is an ID column in the original dataset, weâ€™ll use that ID.\n",
    "\n",
    "The only thing weâ€™ll add here is a tiny bit of cleanup of the offset mappings. They will contain offsets for the question and the context, but once weâ€™re in the post-processing stage we wonâ€™t have any way to know which part of the input IDs corresponded to the context and which part was the question (the sequence_ids() method we used is available for the output of the tokenizer only). So, weâ€™ll set the offsets corresponding to the question to None:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 384\n",
    "stride = 128\n",
    "\n",
    "max_input_length = 256\n",
    "max_target_length = 64\n",
    "\n",
    "\n",
    "prefix1 = \"Given context: \"\n",
    "prefix2 = \"Answer the Question: \"\n",
    "suffix1 = \"\"  # \"Answer: \"\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    context = [prefix1 + doc for doc in examples[\"context\"] ]\n",
    "    questions = [prefix2 + q.strip() + suffix1 for q in examples[\"question\"]]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        context,\n",
    "        questions,\n",
    "        max_length=max_input_length,\n",
    "        truncation=\"only_first\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    answers_text = []\n",
    "    \n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        answer_text = answer[\"text\"][0]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            answers_text.append(\"\")\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "            answers_text.append(answer_text)\n",
    "            \n",
    "    labels = tokenizer(\n",
    "        answers_text, max_length=max_target_length, truncation=True\n",
    "    )\n",
    "\n",
    "    #if (start_positions and end_positions):\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    '''\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "    '''\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5db3f7476654418aed640c050e5d6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10570, 12484)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"validation\"]), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model_name = \"MaRiOrOsSi/t5-base-finetuned-question-answering\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### under the hood of compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will output logits for the start and end positions of the answer in the input IDs, as we saw during our exploration of the question-answering pipeline. The post-processing step will be similar to what we did there, so hereâ€™s a quick reminder of the actions we took:\n",
    "\n",
    "We masked the start and end logits corresponding to tokens outside of the context.\n",
    "We then converted the start and end logits into probabilities using a softmax.\n",
    "We attributed a score to each (start_token, end_token) pair by taking the product of the corresponding two probabilities.\n",
    "We looked for the pair with the maximum score that yielded a valid answer (e.g., a start_token lower than end_token).\n",
    "Here we will change this process slightly because we donâ€™t need to compute actual scores (just the predicted answer). This means we can skip the softmax step. To go faster, we also wonâ€™t score all the possible (start_token, end_token) pairs, but only the ones corresponding to the highest n_best logits (with n_best=20). Since we will skip the softmax, those scores will be logit scores, and will be obtained by taking the sum of the start and end logits (instead of the product, because of the rule \n",
    "log\n",
    "â¡\n",
    "(\n",
    "ï¿½\n",
    "ï¿½\n",
    ")\n",
    "=\n",
    "log\n",
    "â¡\n",
    "(\n",
    "ï¿½\n",
    ")\n",
    "+\n",
    "log\n",
    "â¡\n",
    "(\n",
    "ï¿½\n",
    ")\n",
    "log(ab)=log(a)+log(b)).\n",
    "\n",
    "To demonstrate all of this, we will need some kind of predictions. Since we have not trained our model yet, we are going to use the default model for the QA pipeline to generate some predictions on a small part of the validation set. We can use the same processing function as before; because it relies on the global constant tokenizer, we just have to change that object to the tokenizer of the model we want to use temporarily:m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3830942d14d54d799d19b3704b8dfd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_eval_set = raw_datasets[\"validation\"].select(range(100))\n",
    "#trained_checkpoint =  \"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "eval_set = small_eval_set.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "eval_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_MODEL = \"my_awesome_qa_model_20230617/checkpoint-1000\"\n",
    "trained_checkpoint = \"distilbert-base-cased-distilled-squad\" # QA_MODEL #\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: eval_set_for_model[k]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m eval_set_for_model\u001b[38;5;241m.\u001b[39mcolumn_names}\n",
      "Cell \u001b[0;32mIn[105], line 1\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: \u001b[43meval_set_for_model\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m eval_set_for_model\u001b[38;5;241m.\u001b[39mcolumn_names}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then remove the columns of our eval_set that are not expected by the model, build a batch with all of that small validation set, and pass it through the model. If a GPU is available, we use it to go faster:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#eval_set_for_model.set_format(\"torch\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: eval_set[k]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m eval_set\u001b[38;5;241m.\u001b[39mcolumn_names}\n\u001b[1;32m      9\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m AutoModelForQuestionAnswering\u001b[38;5;241m.\u001b[39mfrom_pretrained(trained_checkpoint)\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m     10\u001b[0m     device\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[0;32mIn[89], line 8\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#eval_set_for_model.set_format(\"torch\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: \u001b[43meval_set\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m eval_set\u001b[38;5;241m.\u001b[39mcolumn_names}\n\u001b[1;32m      9\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m AutoModelForQuestionAnswering\u001b[38;5;241m.\u001b[39mfrom_pretrained(trained_checkpoint)\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m     10\u001b[0m     device\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "#eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "#eval_set_for_model.set_format(\"torch\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "batch = {k: eval_set[k].to(device) for k in eval_set.column_names}\n",
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = trained_model(**batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Trainer will give us predictions as NumPy arrays, we grab the start and end logits and convert them to that format:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits.cpu().numpy()\n",
    "end_logits = outputs.end_logits.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to find the predicted answer for each example in our small_eval_set. One example may have been split into several features in eval_set, so the first step is to map each example in small_eval_set to the corresponding features in eval_set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "example_to_features = collections.defaultdict(list)\n",
    "for idx, feature in enumerate(eval_set):\n",
    "    example_to_features[feature[\"example_id\"]].append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in hand, we can really get to work by looping through all the examples and, for each example, through all the associated features. As we said before, weâ€™ll look at the logit scores for the n_best start logits and end logits, excluding positions that give:\n",
    "\n",
    "* An answer that wouldnâ€™t be inside the context\n",
    "* An answer with negative length\n",
    "* An answer that is too long (we limit the possibilities at max_answer_length=30)\n",
    "\n",
    "Once we have all the scored possible answers for one example, we just pick the one with the best logit score:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "predicted_answers = []\n",
    "\n",
    "for example in small_eval_set:\n",
    "    example_id = example[\"id\"]\n",
    "    context = example[\"context\"]\n",
    "    answers = []\n",
    "\n",
    "    for feature_index in example_to_features[example_id]:\n",
    "        start_logit = start_logits[feature_index]\n",
    "        end_logit = end_logits[feature_index]\n",
    "        offsets = eval_set[\"offset_mapping\"][feature_index]\n",
    "\n",
    "        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                # Skip answers that are not fully in the context\n",
    "                if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                    continue\n",
    "                # Skip answers with a length that is either < 0 or > max_answer_length.\n",
    "                if (\n",
    "                    end_index < start_index\n",
    "                    or end_index - start_index + 1 > max_answer_length\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                answers.append(\n",
    "                    {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "    predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final format of the predicted answers is the one that will be expected by the metric we will use. As usual, we can load it with the help of the ðŸ¤— Evaluate library:\n",
    "\n",
    "This metric expects the predicted answers in the format we saw above (a list of dictionaries with one key for the ID of the example and one key for the predicted text) and the theoretical answers in the format below (a list of dictionaries with one key for the ID of the example and one key for the possible answers):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "#metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "theoretical_answers = [\n",
    "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}\n",
      "{'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}\n"
     ]
    }
   ],
   "source": [
    "# We can now check that we get sensible results by looking at the first element of both lists:\n",
    "\n",
    "print(predicted_answers[0])\n",
    "print(theoretical_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 83.0, 'f1': 88.25000000000004}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not too bad! Now letâ€™s have a look at the score the metric gives us:\n",
    "\n",
    "metric.compute(predictions=predicted_answers, references=theoretical_answers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, thatâ€™s rather good considering that according to its paper DistilBERT fine-tuned on SQuAD obtains 79.1 and 86.9 for those scores on the whole dataset.\n",
    "\n",
    "Now letâ€™s put everything we just did in a compute_metrics() function that we will use in the Trainer. Normally, that compute_metrics() function only receives a tuple eval_preds with logits and labels. Here we will need a bit more, as we have to look in the dataset of features for the offset and in the dataset of examples for the original contexts, so we wonâ€™t be able to use this function to get regular evaluation results during training. We will only use it at the end of training to check the results.\n",
    "\n",
    "The compute_metrics() function groups the same steps as before; we just add a small check in case we donâ€™t come up with any valid answers (in which case we predict an empty string).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8WhoMeQM7vV"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3232787f20424776b1f07f22e2e6bc0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 83.0, 'f1': 88.25000000000004}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(start_logits, end_logits, eval_set, small_eval_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wfLNIWSM7vV"
   },
   "source": [
    "<Tip>\n",
    "\n",
    "If you aren't familiar with finetuning a model with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)!\n",
    "\n",
    "</Tip>\n",
    "\n",
    "You're ready to start training your model now! Load DistilBERT with [AutoModelForQuestionAnswering](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForQuestionAnswering):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "T15JBGCSM7vV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\") # pretrained_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0lHk45TM7vV"
   },
   "source": [
    "Once this is done, we can define our TrainingArguments. As we said when we defined our function to compute the metric, we wonâ€™t be able to have a regular evaluation loop because of the signature of the compute_metrics() function. We could write our own subclass of Trainer to do this (an approach you can find in the question answering example script), but thatâ€™s a bit too long for this section. Instead, we will only evaluate the model at the end of training here and show you how to do a regular evaluation in â€œA custom training loopâ€ below.\n",
    "\n",
    "`This is really where the Trainer API shows its limits and the ðŸ¤— Accelerate library shines: customizing the class to a specific use case can be painful, but tweaking a fully exposed training loop is easy.`\n",
    "\n",
    "\n",
    "\n",
    "At this point, only three steps remain:\n",
    "\n",
    "1. Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model).\n",
    "2. Pass the training arguments to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, tokenizer, and data collator.\n",
    "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "M1jN_-xyM7vV"
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = \"my_awesome_qa_model_20230617\"\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-squad\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weâ€™ve seen most of these before: we set some hyperparameters (like the learning rate, the number of epochs we train for, and some weight decay) and indicate that we want to save the model at the end of every epoch, skip evaluation, and upload our results to the Model Hub. We also enable mixed-precision training with fp16=True, as it can speed up the training nicely on a recent GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "M1jN_-xyM7vV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 04:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.735640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.016100</td>\n",
       "      <td>1.805603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.016100</td>\n",
       "      <td>1.755508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.585100</td>\n",
       "      <td>1.934661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.585100</td>\n",
       "      <td>1.987095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.7263977905273438, metrics={'train_runtime': 273.0865, 'train_samples_per_second': 73.237, 'train_steps_per_second': 4.577, 'total_flos': 1959796500480000.0, 'train_loss': 0.7263977905273438, 'epoch': 5.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while the training happens, each time the model is saved (here, every epoch) it is uploaded to the Hub in the background. This way, you will be able to to resume your training on another machine if necessary. The whole training takes a while (a little over an hour on a Titan RTX), so you can grab a coffee or reread some of the parts of the course that youâ€™ve found more challenging while it proceeds. Also note that as soon as the first epoch is finished, you will see some weights uploaded to the Hub and you can start playing with your model on its page.\n",
    "\n",
    "\n",
    "Once the training is complete, we can finally evaluate our model (and pray we didnâ€™t spend all that compute time on nothing). The predict() method of the Trainer will return a tuple where the first elements will be the predictions of the model (here a pair with the start and end logits). We send this to our compute_metrics() function:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, _, _ = trainer.predict(validation_dataset)\n",
    "start_logits, end_logits = predictions\n",
    "compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bj6JhEX9M7vV"
   },
   "source": [
    "Once training is completed, share your model to the Hub with the [push_to_hub()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub) method so everyone can use your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "id": "S555bIyJM7vV",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/trainer.py:3578\u001b[0m, in \u001b[0;36mTrainer.push_to_hub\u001b[0;34m(self, commit_message, blocking, **kwargs)\u001b[0m\n\u001b[1;32m   3575\u001b[0m \u001b[38;5;66;03m# If a user calls manually `push_to_hub` with `self.args.push_to_hub = False`, we try to create the repo but\u001b[39;00m\n\u001b[1;32m   3576\u001b[0m \u001b[38;5;66;03m# it might fail.\u001b[39;00m\n\u001b[1;32m   3577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 3578\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_git_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3580\u001b[0m model_name \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n",
      "File \u001b[0;32m~/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/trainer.py:3431\u001b[0m, in \u001b[0;36mTrainer.init_git_repo\u001b[0;34m(self, at_init)\u001b[0m\n\u001b[1;32m   3429\u001b[0m     repo_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_model_id\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m repo_name:\n\u001b[0;32m-> 3431\u001b[0m     repo_name \u001b[38;5;241m=\u001b[39m \u001b[43mget_full_repo_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;66;03m# Make sure the repo exists.\u001b[39;00m\n\u001b[1;32m   3434\u001b[0m create_repo(repo_name, token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_token, private\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_private_repo, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/bioblp-env/lib/python3.9/site-packages/transformers/utils/hub.py:795\u001b[0m, in \u001b[0;36mget_full_repo_name\u001b[0;34m(model_id, organization, token)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_full_repo_name\u001b[39m(model_id: \u001b[38;5;28mstr\u001b[39m, organization: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, token: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 795\u001b[0m         username \u001b[38;5;241m=\u001b[39m \u001b[43mwhoami\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    796\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00musername\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/bioblp-env/lib/python3.9/site-packages/huggingface_hub/hf_api.py:827\u001b[0m, in \u001b[0;36mHfApi.whoami\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwhoami\u001b[39m(\u001b[38;5;28mself\u001b[39m, token: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;124;03m    Call HF API to know \"whoami\".\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;124;03m            not provided.\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    825\u001b[0m     r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    826\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/whoami-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 827\u001b[0m         headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_hf_headers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# If `token` is provided and not `None`, it will be used by default.\u001b[39;49;00m\n\u001b[1;32m    829\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Otherwise, the token must be retrieved from cache or env variable.\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    834\u001b[0m         hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/.conda/envs/bioblp-env/lib/python3.9/site-packages/huggingface_hub/hf_api.py:4205\u001b[0m, in \u001b[0;36mHfApi._build_hf_headers\u001b[0;34m(self, token, is_write_action, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   4202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4203\u001b[0m     \u001b[38;5;66;03m# Cannot do `token = token or self.token` as token can be `False`.\u001b[39;00m\n\u001b[1;32m   4204\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken\n\u001b[0;32m-> 4205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_hf_headers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_write_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_write_action\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4210\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4211\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/bioblp-env/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    118\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/bioblp-env/lib/python3.9/site-packages/huggingface_hub/utils/_headers.py:117\u001b[0m, in \u001b[0;36mbuild_hf_headers\u001b[0;34m(token, is_write_action, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03mBuild headers dictionary to send in a HF Hub call.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m        If `token=True` but token is not saved locally.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Get auth token to send\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m token_to_send \u001b[38;5;241m=\u001b[39m \u001b[43mget_token_to_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m _validate_token_to_send(token_to_send, is_write_action\u001b[38;5;241m=\u001b[39mis_write_action)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Combine headers\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/bioblp-env/lib/python3.9/site-packages/huggingface_hub/utils/_headers.py:149\u001b[0m, in \u001b[0;36mget_token_to_send\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cached_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    150\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken is required (`token=True`), but no token found. You\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m need to provide a token or be logged in to Hugging Face with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `huggingface-cli login` or `huggingface_hub.login`. See\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://huggingface.co/settings/tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         )\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_token\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Case implicit use of the token is forbidden by env variable\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens."
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwj0ryjNM7vV"
   },
   "source": [
    "\n",
    "`https://huggingface.co/learn/nlp-course/chapter7/7?fw=pt#a-custom-training-loop|`\n",
    "\n",
    "<Tip>\n",
    "   \n",
    "For a more in-depth example of how to finetune a model for question answering, take a look at the corresponding\n",
    "[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)\n",
    "or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb).\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"huggingface-course/bert-finetuned-squad\"\n",
    "question_answerer = pipeline(\"question-answering\", model=model_checkpoint)\n",
    "\n",
    "context = \"\"\"\n",
    "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back ðŸ¤— Transformers?\"\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hs8AI4S_M7vV"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMLSN6cvM7vV"
   },
   "source": [
    "Evaluation for question answering requires a significant amount of postprocessing. To avoid taking up too much of your time, this guide skips the evaluation step. The [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) still calculates the evaluation loss during training so you're not completely in the dark about your model's performance.\n",
    "\n",
    "If have more time and you're interested in how to evaluate your model for question answering, take a look at the [Question answering](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing) chapter from the ðŸ¤— Hugging Face Course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRITICAL! https://huggingface.co/learn/nlp-course/chapter7/7?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 50\n",
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] how many animals did mu ying capture? [SEP] patricia ebrey writes that tibet, like joseon korea and other neighboring states to the ming, settled for its tributary status while there were no troops or governors of ming china stationed in its territory. laird writes that \" after the mongol troops left tibet, no ming troops replaced them. \" wang and nyima state that, despite the fact that the ming refrained from sending troops to subdue tibet and refrained from garrisoning ming troops there, these measures were unnecessary so long as the ming court upheld close ties with tibetan vassals and their forces. however, there were instances in the 14th century when the hongwu emperor did use military force to quell unrest in tibet. john d. langlois writes that there was unrest in tibet and western sichuan, which the marquis mu ying ( [UNK] è‹± ) was commissioned to quell in november 1378 after he established a taozhou garrison in gansu. langlois notes that by october 1379, mu ying had allegedly captured 30, 000 tibetan prisoners and 200, 000 domesticated animals. yet invasion went both ways ; the ming general qu neng, under the command of lan yu, was ordered to repel a tibetan assault into sichuan in 1390. [SEP]'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = squad[\"train\"][0][\"context\"]\n",
    "question = squad[\"train\"][0][\"question\"]\n",
    "\n",
    "inputs = tokenizer(question, context)\n",
    "tokenizer.decode(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] how many animals did mu ying capture? [SEP] patricia ebrey writes that tibet, like joseon korea and other neighboring states to the ming, settled for its tributary status while there were no troops or governors of ming china stationed in its territory. laird writes that \" after the mongol troops left tibet, no ming troops replaced them. \" wang and nyima state that, despite the fact that the ming refrained from sending troops to subdue tibet and refrained from garrisoning ming troops there, these measures were unnecessary so long as the ming court upheld close ties with tibetan vassals and their forces. however, there were instances in the 14th century when the hongwu emperor did use military force to quell unrest in tibet. john d. langlois writes that there was unrest in tibet and western sichuan, which the marquis mu ying ( [UNK] è‹± ) was commissioned to quell in november 1378 after he established a taozhou garrison in gansu. langlois notes that by october [SEP]\n",
      "[CLS] how many animals did mu ying capture? [SEP] langlois writes that there was unrest in tibet and western sichuan, which the marquis mu ying ( [UNK] è‹± ) was commissioned to quell in november 1378 after he established a taozhou garrison in gansu. langlois notes that by october 1379, mu ying had allegedly captured 30, 000 tibetan prisoners and 200, 000 domesticated animals. yet invasion went both ways ; the ming general qu neng, under the command of lan yu, was ordered to repel a tibetan assault into sichuan in 1390. [SEP]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=200,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    ")\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e04ab61b454e999d8811896f02b15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "small_eval_set = squad[\"test\"].select(range(100))\n",
    "trained_checkpoint = QA_MODEL #\"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "eval_set = small_eval_set.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=squad[\"test\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "eval_set_for_model.set_format(\"torch\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = trained_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits.cpu().numpy()\n",
    "end_logits = outputs.end_logits.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to find the predicted answer for each example in our small_eval_set. One example may have been split into several features in eval_set, so the first step is to map each example in small_eval_set to the corresponding features in eval_set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "example_to_features = collections.defaultdict(list)\n",
    "for idx, feature in enumerate(eval_set):\n",
    "    example_to_features[feature[\"example_id\"]].append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in hand, we can really get to work by looping through all the examples and, for each example, through all the associated features. As we said before, weâ€™ll look at the logit scores for the n_best start logits and end logits, excluding positions that give:\n",
    "\n",
    "An answer that wouldnâ€™t be inside the context\n",
    "An answer with negative length\n",
    "An answer that is too long (we limit the possibilities at max_answer_length=30)\n",
    "Once we have all the scored possible answers for one example, we just pick the one with the best logit score:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "predicted_answers = []\n",
    "\n",
    "for example in small_eval_set:\n",
    "    example_id = example[\"id\"]\n",
    "    context = example[\"context\"]\n",
    "    answers = []\n",
    "\n",
    "    for feature_index in example_to_features[example_id]:\n",
    "        start_logit = start_logits[feature_index]\n",
    "        end_logit = end_logits[feature_index]\n",
    "        offsets = eval_set[\"offset_mapping\"][feature_index]\n",
    "\n",
    "        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                # Skip answers that are not fully in the context\n",
    "                if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                    continue\n",
    "                # Skip answers with a length that is either < 0 or > max_answer_length.\n",
    "                if (\n",
    "                    end_index < start_index\n",
    "                    or end_index - start_index + 1 > max_answer_length\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                answers.append(\n",
    "                    {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "    predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final format of the predicted answers is the one that will be expected by the metric we will use. As usual, we can load it with the help of the ðŸ¤— Evaluate library:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec0b30c6d90400c921bb31aa19fa473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa424e9dbf5445082601aa925f1422a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric expects the predicted answers in the format we saw above (a list of dictionaries with one key for the ID of the example and one key for the predicted text) and the theoretical answers in the format below (a list of dictionaries with one key for the ID of the example and one key for the possible answers):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "theoretical_answers = [\n",
    "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '573383494776f41900660c41', 'prediction_text': 'Fighting Irish football team, especially under the guidance of the legendary coach Knute Rockne'}\n",
      "{'id': '573383494776f41900660c41', 'answers': {'text': ['its Fighting Irish football team'], 'answer_start': [62]}}\n"
     ]
    }
   ],
   "source": [
    "# We can now check that we get sensible results by looking at the first element of both lists:\n",
    "\n",
    "print(predicted_answers[0])\n",
    "print(theoretical_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 55.0, 'f1': 65.96781045751634}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=predicted_answers, references=theoretical_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d5bdb635a541b29d1b669fa1ed0fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 55.0, 'f1': 65.96781045751634}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(start_logits, end_logits, eval_set, small_eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this above bit again following teh course guide. I suspect teh stride parameter would play an important role. We ignored this in the training loop earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wa7zsvXiM7vV"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRtKJTDpM7vW"
   },
   "source": [
    "Great, now that you've finetuned a model, you can use it for inference!\n",
    "\n",
    "Come up with a question and some context you'd like the model to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "W-ijqSxrM7vW"
   },
   "outputs": [],
   "source": [
    "question = \"How many programming languages does BLOOM support?\"\n",
    "context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many countries are there in the world?\"\n",
    "context = \"There are 199 countries on earth and 23 on the moon.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LATqxbynM7vW"
   },
   "source": [
    "The simplest way to try out your finetuned model for inference is to use it in a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). Instantiate a `pipeline` for question answering with your model, and pass your text to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "zLDNk8MPM7vW",
    "outputId": "4a4e12ed-277e-4746-e61b-d2b20a1bdccc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.33080992102622986, 'start': 10, 'end': 13, 'answer': '199'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "QA_MODEL = \"my_awesome_qa_model_20230617/checkpoint-1000\"\n",
    "question_answerer = pipeline(\"question-answering\", model=QA_MODEL)\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWVAfFClM7vW"
   },
   "source": [
    "You can also manually replicate the results of the `pipeline` if you'd like:\n",
    "\n",
    "Tokenize the text and return PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "00sc0u9gM7vZ"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(QA_MODEL)\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1Ct6j3FM7vZ"
   },
   "source": [
    "Pass your inputs to the model and return the `logits`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2LVAfs-M7vZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"my_awesome_qa_model_20230617\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Phxxprz5M7vZ"
   },
   "source": [
    "Get the highest probability from the model output for the start and end positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67cRX6NMM7va"
   },
   "outputs": [],
   "source": [
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-hYHutwM7va"
   },
   "source": [
    "Decode the predicted tokens to get the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skIzO3IqM7va",
    "outputId": "c16a25f3-df4f-4db6-d830-ccb0085d8780"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'176 billion parameters and can generate text in 46 languages natural languages and 13'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".conda-bioblp-env [Python]",
   "language": "python",
   "name": "conda-env-.conda-bioblp-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
